{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation using UNet on Carvana Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four modules:\n",
    "1. DataLoader\n",
    "2. UNet Model\n",
    "3. Utils\n",
    "4. Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class CarvanaDataset:\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir) # get list of all the images in the image directory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is my UNet model architecture.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(Conv2dSamePadding, self).__init__(*args, **kwargs)\n",
    "        self.zero_pad_2d = nn.ZeroPad2d(reduce(__add__,\n",
    "            [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_size[::-1]]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return  self._conv_forward(self.zero_pad_2d(input), self.weight, self.bias)\n",
    "    \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet,self).__init__()\n",
    "        # Encoder Layers\n",
    "        # self.layer1 = self.doubleConv(1,64)\n",
    "        self.layer1 = self.doubleConv(3,64)         # changed the input channels\n",
    "        self.l2 = nn.MaxPool2d(2,2,0)\n",
    "        self.l3 = self.doubleConv(64,128)\n",
    "        self.l4 = nn.MaxPool2d(2,2,0)\n",
    "        self.l5 = self.doubleConv(128,256)\n",
    "        self.l6 = nn.MaxPool2d(2,2,0)\n",
    "        self.l7 = self.doubleConv(256,512)\n",
    "        self.l8 = nn.MaxPool2d(2,2,0)\n",
    "        self.l9 = self.doubleConv(512,1024)\n",
    "        # Decoder Layers\n",
    "        self.l10 = nn.ConvTranspose2d(1024,512,2,2,0)\n",
    "        self.l11 = self.doubleConv(1024,512)\n",
    "        self.l12 = nn.ConvTranspose2d(512,256,2,2,0)\n",
    "        self.l13 = self.doubleConv(512,256)\n",
    "        self.l14 = nn.ConvTranspose2d(256,128,2,2,0)\n",
    "        self.l15 = self.doubleConv(256,128)\n",
    "        self.l16 = nn.ConvTranspose2d(128,64,2,2,0)\n",
    "        self.l17 = self.doubleConv(128,64)\n",
    "        # self.l18 = nn.ConvTranspose2d(64,2,1,1,0)\n",
    "        self.l18 = nn.ConvTranspose2d(64,1,1,1,0)       # Changed the output channels\n",
    "        \n",
    "\n",
    "    def doubleConv(self, in_channel, out_channel):\n",
    "        return nn.Sequential(\n",
    "            # nn.Conv2d(in_channel,out_channel,3,1,0),        # VALID CONVOLUTION\n",
    "            # nn.Conv2d(out_channel,out_channel,3,1,0),       # VALID CONVOLUTION\n",
    "            Conv2dSamePadding(in_channel,out_channel,3,1,0),      # SAME CONVOLUTION\n",
    "            Conv2dSamePadding(out_channel,out_channel,3,1,0),     # SAME CONVOLUTION\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def concatenate(self, tensor, target_tensor):\n",
    "        delta = int((tensor.shape[2] - target_tensor.shape[2])/2)\n",
    "        tensor = tensor[:,:, delta:tensor.shape[2]-delta, delta:tensor.shape[2]-delta]\n",
    "        return torch.cat((tensor,target_tensor),1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''Need to add batch normalization'''\n",
    "        # Encoder\n",
    "        x1 = self.layer1(input)\n",
    "        x2 = self.l2(x1)\n",
    "        x3 = self.l3(x2)\n",
    "        x4 = self.l4(x3)\n",
    "        x5 = self.l5(x4)\n",
    "        x6 = self.l6(x5)\n",
    "        x7 = self.l7(x6)\n",
    "        x8 = self.l8(x7)\n",
    "        x9 = self.l9(x8)\n",
    "        \n",
    "        # Decoder\n",
    "        x10 = self.l10(x9)\n",
    "        x11 = self.l11(self.concatenate(x7,x10))\n",
    "        x12 = self.l12(x11)\n",
    "        x13 = self.l13(self.concatenate(x5,x12))\n",
    "        x14 = self.l14(x13)\n",
    "        x15 = self.l15(self.concatenate(x3,x14))\n",
    "        x16 = self.l16(x15)\n",
    "        x17 = self.l17(self.concatenate(x1,x16))\n",
    "        x18 = self.l18(x17)\n",
    "\n",
    "        return x18\n",
    "\n",
    "def test():    \n",
    "    model = UNet()\n",
    "    x = torch.rand(1,3,512,512)\n",
    "    print(\"shape of x: \", x.shape)\n",
    "    y = model(x)\n",
    "    print(\"shape of y: \", y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()\n",
    "    return dice_score\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"saved_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import check_accuracy, save_predictions_as_imgs\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "IMAGE_HEIGHT = 512 # 160  # 1280 originally\n",
    "IMAGE_WIDTH = 512 # 240  # 1918 originally\n",
    "TRAIN_IMG_DIR = \"data/train_images/\"\n",
    "TRAIN_MASK_DIR = \"data/train_masks/\"\n",
    "VAL_IMG_DIR = \"data/val_images/\"\n",
    "VAL_MASK_DIR = \"data/val_masks/\"\n",
    "LOAD_MODEL = False\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "# model_path = 'models/train_02_d = '+str(discount)+'.pt'\n",
    "model_path = \"my_checkpoint.pth.tar\"\n",
    "lossLog = SummaryWriter()\n",
    "\n",
    "def main():\n",
    "    #--------------------------------------------------------\n",
    "    '''\n",
    "    Data Augmentation Pipeline:\n",
    "    1. First of all, we define the train and validation transforms.\n",
    "    2. Then we pass this function to the dataset class.\n",
    "    '''\n",
    "    '''Augment the data using the albumentation library'''\n",
    "    train_transform = A.Compose([A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "                                 A.Rotate(limit=35, p=1.0),\n",
    "                                 A.HorizontalFlip(p=0.5),\n",
    "                                 A.VerticalFlip(p=0.1),\n",
    "                                 A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                                             std=[1.0, 1.0, 1.0],\n",
    "                                             max_pixel_value=255.0),\n",
    "                                 ToTensorV2()\n",
    "                                ])\n",
    "\n",
    "    val_transform = A.Compose([A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "                               A.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                                           std=[1.0, 1.0, 1.0],\n",
    "                                           max_pixel_value=255.0),\n",
    "                               ToTensorV2()\n",
    "                               ])\n",
    "    \n",
    "    train_ds = CarvanaDataset(image_dir=TRAIN_IMG_DIR, mask_dir=TRAIN_MASK_DIR, transform=train_transform)\n",
    "    val_ds = CarvanaDataset(image_dir=VAL_IMG_DIR, mask_dir=VAL_MASK_DIR, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY,shuffle=False)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------\n",
    "    # 2. Create the Model, define the Loss Function and the Optimizer.\n",
    "    # model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    model = UNet().to(DEVICE)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        print(\"--- Training from the last checkpoint\")\n",
    "        checkpoint = torch.load(model_path)\n",
    "        \n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        current_epoch = checkpoint[\"epoch\"]\n",
    "        # seed = checkpoint['seed']\n",
    "\n",
    "    # 3. Start the training\n",
    "    for epoch in tqdm(range(current_epoch, NUM_EPOCHS)):\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "            \n",
    "            data = data.to(device=DEVICE)  # [8, 3, 572, 572]\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)  # [8, 1, 572, 572]\n",
    "\n",
    "            # forward\n",
    "            predictions = model(data)       # [8, 1, 572, 572]\n",
    "            loss = loss_fn(predictions, targets)    # Need to check this line\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        # Saving the model\n",
    "        print(\"=> Saving Checkpoint\")\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "                      \"optimizer\":optimizer.state_dict(),\n",
    "                      \"epoch\": epoch,\n",
    "                      # \"seed\": get_seed\n",
    "                      }\n",
    "        torch.save(checkpoint, model_path)\n",
    "\n",
    "        # Save the data to tensorboard\n",
    "        lossLog.add_scalar(\"Training loss per epoch\", loss, epoch)\n",
    "        # check accuracy\n",
    "        check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        # print some examples to a folder\n",
    "        save_predictions_as_imgs(val_loader, model, folder=\"saved_images/\", device=DEVICE)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
